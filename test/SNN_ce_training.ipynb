{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wAsFq8i9Maj6"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if \"mla-prj-23-mla-prj-12-gu1\" not in os.listdir(\"./\"):\n",
        "  !git clone https://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@github.com/MLinApp-polito/mla-prj-23-mla-prj-12-gu1.git\n",
        "\n",
        "!pip install snntorch"
      ],
      "metadata": {
        "id": "RkUljZEOMg1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mla-prj-23-mla-prj-12-gu1/"
      ],
      "metadata": {
        "id": "vPm6vsfAN84k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import csv\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import snntorch as snn\n",
        "from snntorch import utils, surrogate\n",
        "import snntorch.functional as SF\n",
        "import json"
      ],
      "metadata": {
        "id": "5UW724BqMHx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Utilities for data loading and processing."
      ],
      "metadata": {
        "id": "wAsFq8i9Maj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pad_data(data):\n",
        "  miR_data = data\n",
        "  c_int = math.ceil(np.sqrt(len(miR_data[0])))\n",
        "  pad = c_int ** 2 - len(miR_data[0])\n",
        "  pad_width = (0, pad)\n",
        "\n",
        "  padded_miR_data = np.zeros((miR_data.shape[0], miR_data.shape[1] + pad_width[1]))\n",
        "\n",
        "  for i in range(len(miR_data)):\n",
        "    padded_miR_data[i] = np.pad(miR_data[i], pad_width, mode='constant')\n",
        "\n",
        "  # reshape shape[1] into (c_int, c_int)\n",
        "\n",
        "  dim = int(np.sqrt(len(padded_miR_data[0])))\n",
        "  padded_miR_data = padded_miR_data.reshape((padded_miR_data.shape[0],1, dim, dim))\n",
        "\n",
        "  return padded_miR_data\n",
        "\n",
        "def build_dataloader(miR_data, num_miR_label, padded_data, batch_size=404):\n",
        "\n",
        "    if padded_data:\n",
        "        miR_data = add_pad_data(miR_data)\n",
        "\n",
        "    train_data, val_data, train_label, val_label = train_test_split(miR_data, num_miR_label, test_size=0.20, random_state=42)\n",
        "\n",
        "    miR_train = torch.Tensor(train_data)\n",
        "    miR_train = miR_train.unsqueeze(1)\n",
        "    miR_train_label = torch.LongTensor(train_label)\n",
        "    miR_dataset_train = TensorDataset(miR_train, miR_train_label)\n",
        "\n",
        "    miR_val = torch.Tensor(val_data)\n",
        "    miR_val = miR_val.unsqueeze(1)\n",
        "    miR_val_label = torch.LongTensor(val_label)\n",
        "    miR_dataset_val = TensorDataset(miR_val, miR_val_label)\n",
        "\n",
        "    train_loader = DataLoader(miR_dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(miR_dataset_val, batch_size=batch_size)\n",
        "\n",
        "    if padded_data:\n",
        "        num_inputs = train_data.shape[2] ** 2\n",
        "    else:\n",
        "        num_inputs = train_data.shape[1]\n",
        "\n",
        "    return num_inputs, train_loader, test_loader\n",
        "\n",
        "def normalize(data, method='zscore'):\n",
        "    if method == \"zscore\":\n",
        "        return scipy.stats.zscore(data, axis=1)\n",
        "\n",
        "    # log2 normalization\n",
        "    elif method==\"log2\":\n",
        "        data = data + abs(np.min(data)) + 0.001\n",
        "        return np.log2(data)\n",
        "\n",
        "    # normalization between [0, 255]\n",
        "    else:\n",
        "       return (data - np.min(data)) / (np.max(data) - np.min(data)) * 255\n",
        "\n",
        "def extract_label(file_name, verbose=False):\n",
        "    data = {}\n",
        "    label = []\n",
        "    with open(file_name, \"r\") as fin:\n",
        "        reader = csv.reader(fin, delimiter=',')\n",
        "        first = True\n",
        "        for row in reader:\n",
        "            lbl = row[2]\n",
        "            if first or \"TARGET\" in lbl:\n",
        "                first = False\n",
        "                continue\n",
        "            lbl = lbl.replace(\"TCGA-\",\"\")\n",
        "\n",
        "            label.append(lbl)\n",
        "            if lbl in data.keys():\n",
        "                data[lbl] += 1\n",
        "            else:\n",
        "                data[lbl] = 1\n",
        "    if verbose:\n",
        "        print(f\"Number of classes in the dataset = {len(data)}\")\n",
        "        pprint.pprint(data, indent=4)\n",
        "\n",
        "    return label\n",
        "\n",
        "def create_dictionary(labels):\n",
        "    dictionary = {}\n",
        "    class_names = np.unique(labels)\n",
        "    for i, name in enumerate(class_names):\n",
        "        dictionary[name] = i\n",
        "    return dictionary\n",
        "\n",
        "def label_processing(labels):\n",
        "    new_miRna_label = []\n",
        "    dictionary = create_dictionary(labels)\n",
        "    for i in labels:\n",
        "        new_miRna_label.append(dictionary[i])\n",
        "    return new_miRna_label\n",
        "\n",
        "def top_10_dataset(miR_data, miR_label):\n",
        "  occ = dict({k: 0 for k in set(miR_label)})\n",
        "\n",
        "  for i in range(len(miR_label)):\n",
        "    occ[miR_label[i]] += 1\n",
        "\n",
        "  top_10_class = sorted(occ, key=occ.get,reverse=True)[:10]\n",
        "\n",
        "  list_top_10_train = []\n",
        "  list_top_10_labels = []\n",
        "\n",
        "  for i in range(len(miR_label)):\n",
        "    if miR_label[i] in top_10_class:\n",
        "      list_top_10_labels.append(miR_label[i])\n",
        "\n",
        "  for i in range(miR_data.shape[0]):\n",
        "    if miR_label[i] in top_10_class:\n",
        "      list_top_10_train.append(miR_data[i])\n",
        "\n",
        "  miR_data_reduced = np.stack(list_top_10_train, axis=0)\n",
        "  miR_label_reduced = list_top_10_labels\n",
        "\n",
        "  num_miR_label_reduced = label_processing(miR_label_reduced)\n",
        "\n",
        "  return miR_data_reduced, miR_label_reduced, num_miR_label_reduced\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "GGf6unq5MZLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Utilities function for network training."
      ],
      "metadata": {
        "id": "k-e19M1BMB2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(net, num_steps, data):\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps):\n",
        "      spk_out, mem_out = net(data)\n",
        "      spk_rec.append(spk_out)\n",
        "      mem_rec.append(mem_out)\n",
        "\n",
        "  return torch.stack(spk_rec), torch.stack(mem_rec)\n",
        "\n",
        "def test_accuracy(train_loader, net, num_steps, device):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    net.eval()\n",
        "\n",
        "    train_loader = iter(train_loader)\n",
        "    for data, targets in train_loader:\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        spk_rec, _ = forward_pass(net, num_steps, data)\n",
        "\n",
        "\n",
        "        acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "  return acc/total\n",
        "\n",
        "def build_layer(layer_type, beta, grad, num_outputs, output=False):\n",
        "    if layer_type==\"leaky\":\n",
        "        if output==True:\n",
        "            return snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, output=output, threshold=0.4)\n",
        "        else:\n",
        "            return snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, threshold=0.4)\n",
        "\n",
        "    if layer_type==\"lapicque\":\n",
        "        if output==True:\n",
        "            return snn.Lapicque(beta=beta, spike_grad=grad, init_hidden=True, output=output, threshold=0.4)\n",
        "        else:\n",
        "            return snn.Lapicque(beta=beta, spike_grad=grad, init_hidden=True, threshold=0.4)\n",
        "\n",
        "    if layer_type==\"rleaky\":\n",
        "        if output==True:\n",
        "            return snn.RLeaky(beta=beta, spike_grad=grad, init_hidden=True, output=output, linear_features=num_outputs, threshold=0.4)\n",
        "        else:\n",
        "            return snn.RLeaky(beta=beta, spike_grad=grad, init_hidden=True, linear_features=num_outputs, threshold=0.4)\n",
        "\n",
        "def get_cnn_dimension(input_size, params_cnn):\n",
        "    conv1_out = ((input_size - 1 * (params_cnn['wd1'] - 1) -1) + 1)\n",
        "    conv1_out = int(conv1_out)\n",
        "\n",
        "    s1 = (((conv1_out - 1 * (params_cnn['h1'] - 1) -1)/params_cnn['h1']) + 1)\n",
        "    s1 = int(s1)\n",
        "\n",
        "    conv2_out = ((s1 - 1 * (params_cnn['wd2'] - 1)-1) + 1)\n",
        "    conv2_out = int(conv2_out)\n",
        "\n",
        "    s2 = (((conv2_out - 1 * (params_cnn['h2'] -1 ) -1) / params_cnn['h2'] ) + 1)\n",
        "    s2 = int(s2)\n",
        "\n",
        "    conv3_out = ((s2 - 1 * (params_cnn['wd3'] - 1)-1) + 1)\n",
        "    conv3_out = int(conv3_out)\n",
        "\n",
        "    s3_dec = (((conv3_out - 1 * (params_cnn['h3'] - 1 ) -1) / params_cnn['h3']) + 1)\n",
        "    if s3_dec < 1:\n",
        "        s3 = 1\n",
        "    else:\n",
        "        s3 = math.floor(s3_dec)\n",
        "\n",
        "    if s3 == 0:\n",
        "        s3 = 1\n",
        "    return s1, s2, s3\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "QUmq2_Z8M57y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data loading and network training."
      ],
      "metadata": {
        "id": "Up-WI-v9mPeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "miR_label = extract_label(\"./dataset/tcga_mir_label.csv\")\n",
        "miR_data = np.genfromtxt('./dataset/tcga_mir_rpm.csv', delimiter=',')[1:,0:-1]\n",
        "number_to_delete = abs(len(miR_label) - miR_data.shape[0])\n",
        "miR_data = miR_data[number_to_delete:,:]\n",
        "\n",
        "# Convert labels in number\n",
        "num_miR_label = label_processing(miR_label)\n",
        "\n",
        "# Z-score\n",
        "miR_data = normalize(miR_data)\n",
        "\n",
        "assert np.isnan(miR_data).sum() == 0\n",
        "\n",
        "#---Number of classes---#\n",
        "top_10_classes = True\n",
        "padded_data = False\n",
        "\n",
        "if top_10_classes:\n",
        "  n_classes = 10\n",
        "  miR_data, miR_label, num_miR_label = top_10_dataset(miR_data, miR_label)\n",
        "else:\n",
        "  n_classes = np.unique(miR_label).size\n",
        "\n",
        "num_inputs, train_loader, test_loader = build_dataloader(miR_data, num_miR_label, padded_data, batch_size=128)\n",
        "\n",
        "grad = surrogate.fast_sigmoid()"
      ],
      "metadata": {
        "id": "mXoIJkL2Ng-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run one of these cell to define and train the spiking network that comes from the non-spiking architecture CNN_PT_1 (first cell) or the one that comes from CNN_PT_2 (second cell)."
      ],
      "metadata": {
        "id": "SlOvM_6Qmakj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./best_hyperparams/cnn_params_best.json\", \"r\") as f:\n",
        "  params_cnn = json.load(f)\n",
        "\n",
        "s1, s2, s3 = get_cnn_dimension(num_inputs, params_cnn)\n",
        "\n",
        "#with open(\"./best_hyperparams/snn_params_SNN_PT_1_ce_tradeoff.json\", \"r\") as f:\n",
        "with open(\"./best_hyperparams/snn_params_SNN_PT_1_ce_best.json\", \"r\") as f:\n",
        "  params_snn = json.load(f)\n",
        "\n",
        "state_dict = torch.load(\"./trained_models/cnn_best_acc_trained_model.pt\")\n",
        "\n",
        "snn_model = nn.Sequential(\n",
        "  nn.Conv1d(in_channels=1, out_channels=params_cnn['w1'], kernel_size=params_cnn['wd1']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h1']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s1),\n",
        "  nn.Conv1d(in_channels=params_cnn['w1'], out_channels=params_cnn['w2'], kernel_size=params_cnn['wd2']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h2']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s2),\n",
        "  nn.Conv1d(in_channels=params_cnn['w2'], out_channels=params_cnn['w3'], kernel_size=params_cnn['wd3']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h3']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s3),\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(s3*params_cnn['w3'], n_classes),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, n_classes, output=True)\n",
        ")"
      ],
      "metadata": {
        "id": "H_eE-SHwOeYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./best_hyperparams/cnn_params_light.json\", \"r\") as f:\n",
        "  params_cnn = json.load(f)\n",
        "\n",
        "s1, s2, s3 = get_cnn_dimension(num_inputs, params_cnn)\n",
        "\n",
        "#with open(\"./best_hyperparams/snn_params_SNN_PT_2_ce_tradeoff.json\", \"r\") as f:\n",
        "with open(\"./best_hyperparams/snn_params_SNN_PT_2_ce_best.json\", \"r\") as f:\n",
        "  params_snn = json.load(f)\n",
        "\n",
        "state_dict = torch.load(\"./trained_models/cnn_light_trained_model.pt\")\n",
        "\n",
        "snn_model = nn.Sequential(\n",
        "  nn.Conv1d(in_channels=1, out_channels=params_cnn['w1'], kernel_size=params_cnn['wd1']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h1']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s1),\n",
        "  nn.Conv1d(in_channels=params_cnn['w1'], out_channels=params_cnn['w2'], kernel_size=params_cnn['wd2']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h2']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s2),\n",
        "  nn.Conv1d(in_channels=params_cnn['w2'], out_channels=params_cnn['w3'], kernel_size=params_cnn['wd3']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h3']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s3),\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(s3*params_cnn['w3'], n_classes),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, n_classes, output=True)\n",
        ")"
      ],
      "metadata": {
        "id": "B-I9alD2Pl1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training loop"
      ],
      "metadata": {
        "id": "Z8X7gPkfmdJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  snn_model[0].weight = torch.nn.Parameter(state_dict['conv1.weight'])\n",
        "  snn_model[0].bias = torch.nn.Parameter(state_dict['conv1.bias'])\n",
        "  snn_model[3].weight = torch.nn.Parameter(state_dict['conv2.weight'])\n",
        "  snn_model[3].bias = torch.nn.Parameter(state_dict['conv2.bias'])\n",
        "  snn_model[6].weight = torch.nn.Parameter(state_dict['conv3.weight'])\n",
        "  snn_model[6].bias = torch.nn.Parameter(state_dict['conv3.bias'])\n",
        "  snn_model[10].weight = torch.nn.Parameter(state_dict['fc1.weight'])\n",
        "  snn_model[10].bias = torch.nn.Parameter(state_dict['fc1.bias'])\n",
        "\n",
        "snn_model.to(device)\n",
        "print(f\"SNN number of parameters: {count_parameters(snn_model)}\")\n",
        "\n",
        "optimizer = torch.optim.Adam(snn_model.parameters(), lr=params_snn['learning_rate'], betas=(0.9, 0.999))\n",
        "loss_fn = SF.ce_count_loss()\n",
        "\n",
        "num_epochs = 30\n",
        "curr_acc = -np.inf\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(num_epochs):\n",
        "  # Training loop\n",
        "  for data, targets in iter(train_loader):\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      snn_model.train()\n",
        "      spk_rec, _ = forward_pass(snn_model, params_snn['num_steps'], data)\n",
        "\n",
        "      # initialize the loss & sum over time\n",
        "      loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "      # Gradient calculation + weight update\n",
        "      optimizer.zero_grad()\n",
        "      loss_val.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  epoch_acc = test_accuracy(test_loader, snn_model, params_snn['num_steps'], device)\n",
        "  if epoch_acc >= curr_acc:\n",
        "      curr_acc = epoch_acc\n",
        "\n",
        "  print(f\"Epoch [{epoch + 1}/{num_epochs}] Test Accuracy: {epoch_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "h86JeqzNOm1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}