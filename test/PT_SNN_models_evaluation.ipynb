{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if \"mla-prj-23-mla-prj-12-gu1\" not in os.listdir(\"./\"):\n",
        "  !git clone https://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@github.com/MLinApp-polito/mla-prj-23-mla-prj-12-gu1.git\n",
        "\n",
        "!pip install snntorch"
      ],
      "metadata": {
        "id": "v4I3StLlrgTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mla-prj-23-mla-prj-12-gu1/"
      ],
      "metadata": {
        "id": "0EI1Bgknsll1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if \"mla-prj-23-mla-prj-12-gu1\" not in os.listdir(\"./\"):\n",
        "  !git clone https://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@github.com/MLinApp-polito/mla-prj-23-mla-prj-12-gu1.git\n",
        "\n",
        "!pip install snntorch"
      ],
      "metadata": {
        "id": "L8yHdgEprhTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import csv\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import snntorch as snn\n",
        "from snntorch import utils, surrogate\n",
        "import snntorch.functional as SF\n",
        "import json"
      ],
      "metadata": {
        "id": "lCME-tVPrquu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pad_data(data):\n",
        "  miR_data = data\n",
        "  c_int = math.ceil(np.sqrt(len(miR_data[0])))\n",
        "  pad = c_int ** 2 - len(miR_data[0])\n",
        "  pad_width = (0, pad)\n",
        "\n",
        "  padded_miR_data = np.zeros((miR_data.shape[0], miR_data.shape[1] + pad_width[1]))\n",
        "\n",
        "  for i in range(len(miR_data)):\n",
        "    padded_miR_data[i] = np.pad(miR_data[i], pad_width, mode='constant')\n",
        "\n",
        "  # reshape shape[1] into (c_int, c_int)\n",
        "\n",
        "  dim = int(np.sqrt(len(padded_miR_data[0])))\n",
        "  padded_miR_data = padded_miR_data.reshape((padded_miR_data.shape[0],1, dim, dim))\n",
        "\n",
        "  return padded_miR_data\n",
        "\n",
        "def build_dataloader(miR_data, num_miR_label, padded_data, batch_size=404):\n",
        "\n",
        "    if padded_data:\n",
        "        miR_data = add_pad_data(miR_data)\n",
        "\n",
        "    train_data, val_data, train_label, val_label = train_test_split(miR_data, num_miR_label, test_size=0.20, random_state=42)\n",
        "\n",
        "    miR_train = torch.Tensor(train_data)\n",
        "    miR_train = miR_train.unsqueeze(1)\n",
        "    miR_train_label = torch.LongTensor(train_label)\n",
        "    miR_dataset_train = TensorDataset(miR_train, miR_train_label)\n",
        "\n",
        "    miR_val = torch.Tensor(val_data)\n",
        "    miR_val = miR_val.unsqueeze(1)\n",
        "    miR_val_label = torch.LongTensor(val_label)\n",
        "    miR_dataset_val = TensorDataset(miR_val, miR_val_label)\n",
        "\n",
        "    train_loader = DataLoader(miR_dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(miR_dataset_val, batch_size=batch_size)\n",
        "\n",
        "    if padded_data:\n",
        "        num_inputs = train_data.shape[2] ** 2\n",
        "    else:\n",
        "        num_inputs = train_data.shape[1]\n",
        "\n",
        "    return num_inputs, train_loader, test_loader\n",
        "\n",
        "def normalize(data, method='zscore'):\n",
        "    if method == \"zscore\":\n",
        "        return scipy.stats.zscore(data, axis=1)\n",
        "\n",
        "    # log2 normalization\n",
        "    elif method==\"log2\":\n",
        "        data = data + abs(np.min(data)) + 0.001\n",
        "        return np.log2(data)\n",
        "\n",
        "    # normalization between [0, 255]\n",
        "    else:\n",
        "       return (data - np.min(data)) / (np.max(data) - np.min(data)) * 255\n",
        "\n",
        "def extract_label(file_name, verbose=False):\n",
        "    data = {}\n",
        "    label = []\n",
        "    with open(file_name, \"r\") as fin:\n",
        "        reader = csv.reader(fin, delimiter=',')\n",
        "        first = True\n",
        "        for row in reader:\n",
        "            lbl = row[2]\n",
        "            if first or \"TARGET\" in lbl:\n",
        "                first = False\n",
        "                continue\n",
        "            lbl = lbl.replace(\"TCGA-\",\"\")\n",
        "\n",
        "            label.append(lbl)\n",
        "            if lbl in data.keys():\n",
        "                data[lbl] += 1\n",
        "            else:\n",
        "                data[lbl] = 1\n",
        "    if verbose:\n",
        "        print(f\"Number of classes in the dataset = {len(data)}\")\n",
        "        pprint.pprint(data, indent=4)\n",
        "\n",
        "    return label\n",
        "\n",
        "def create_dictionary(labels):\n",
        "    dictionary = {}\n",
        "    class_names = np.unique(labels)\n",
        "    for i, name in enumerate(class_names):\n",
        "        dictionary[name] = i\n",
        "    return dictionary\n",
        "\n",
        "def label_processing(labels):\n",
        "    new_miRna_label = []\n",
        "    dictionary = create_dictionary(labels)\n",
        "    for i in labels:\n",
        "        new_miRna_label.append(dictionary[i])\n",
        "    return new_miRna_label\n",
        "\n",
        "def top_10_dataset(miR_data, miR_label):\n",
        "  occ = dict({k: 0 for k in set(miR_label)})\n",
        "\n",
        "  for i in range(len(miR_label)):\n",
        "    occ[miR_label[i]] += 1\n",
        "\n",
        "  top_10_class = sorted(occ, key=occ.get,reverse=True)[:10]\n",
        "\n",
        "  list_top_10_train = []\n",
        "  list_top_10_labels = []\n",
        "\n",
        "  for i in range(len(miR_label)):\n",
        "    if miR_label[i] in top_10_class:\n",
        "      list_top_10_labels.append(miR_label[i])\n",
        "\n",
        "  for i in range(miR_data.shape[0]):\n",
        "    if miR_label[i] in top_10_class:\n",
        "      list_top_10_train.append(miR_data[i])\n",
        "\n",
        "  miR_data_reduced = np.stack(list_top_10_train, axis=0)\n",
        "  miR_label_reduced = list_top_10_labels\n",
        "\n",
        "  num_miR_label_reduced = label_processing(miR_label_reduced)\n",
        "\n",
        "  return miR_data_reduced, miR_label_reduced, num_miR_label_reduced\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def build_layer(layer_type, beta, grad, num_outputs, output=False):\n",
        "    if layer_type==\"leaky\":\n",
        "        if output==True:\n",
        "            return snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, output=output, threshold=0.4)\n",
        "        else:\n",
        "            return snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, threshold=0.4)\n",
        "\n",
        "    if layer_type==\"lapicque\":\n",
        "        if output==True:\n",
        "            return snn.Lapicque(beta=beta, spike_grad=grad, init_hidden=True, output=output, threshold=0.4)\n",
        "        else:\n",
        "            return snn.Lapicque(beta=beta, spike_grad=grad, init_hidden=True, threshold=0.4)\n",
        "\n",
        "    if layer_type==\"rleaky\":\n",
        "        if output==True:\n",
        "            return snn.RLeaky(beta=beta, spike_grad=grad, init_hidden=True, output=output, linear_features=num_outputs, threshold=0.4)\n",
        "        else:\n",
        "            return snn.RLeaky(beta=beta, spike_grad=grad, init_hidden=True, linear_features=num_outputs, threshold=0.4)\n",
        "\n",
        "def get_cnn_dimension(input_size, params_cnn):\n",
        "    conv1_out = ((input_size - 1 * (params_cnn['wd1'] - 1) -1) + 1)\n",
        "    conv1_out = int(conv1_out)\n",
        "\n",
        "    s1 = (((conv1_out - 1 * (params_cnn['h1'] - 1) -1)/params_cnn['h1']) + 1)\n",
        "    s1 = int(s1)\n",
        "\n",
        "    conv2_out = ((s1 - 1 * (params_cnn['wd2'] - 1)-1) + 1)\n",
        "    conv2_out = int(conv2_out)\n",
        "\n",
        "    s2 = (((conv2_out - 1 * (params_cnn['h2'] -1 ) -1) / params_cnn['h2'] ) + 1)\n",
        "    s2 = int(s2)\n",
        "\n",
        "    conv3_out = ((s2 - 1 * (params_cnn['wd3'] - 1)-1) + 1)\n",
        "    conv3_out = int(conv3_out)\n",
        "\n",
        "    s3_dec = (((conv3_out - 1 * (params_cnn['h3'] - 1 ) -1) / params_cnn['h3']) + 1)\n",
        "    if s3_dec < 1:\n",
        "        s3 = 1\n",
        "    else:\n",
        "        s3 = math.floor(s3_dec)\n",
        "\n",
        "    if s3 == 0:\n",
        "        s3 = 1\n",
        "    return s1, s2, s3\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "YdMXGHuNrxm2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CPZfzfArV6L",
        "outputId": "02bdbd1e-ebb1-4a8e-dbb4-95a7151793da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6214, 1881)\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "miR_label = extract_label(\"./dataset/tcga_mir_label.csv\")\n",
        "miR_data = np.genfromtxt('./dataset/tcga_mir_rpm.csv', delimiter=',')[1:,0:-1]\n",
        "number_to_delete = abs(len(miR_label) - miR_data.shape[0])\n",
        "miR_data = miR_data[number_to_delete:,:]\n",
        "\n",
        "# Convert labels in number\n",
        "num_miR_label = label_processing(miR_label)\n",
        "\n",
        "# Z-score\n",
        "miR_data = normalize(miR_data)\n",
        "\n",
        "assert np.isnan(miR_data).sum() == 0\n",
        "\n",
        "#---Number of classes---#\n",
        "top_10_classes = True\n",
        "padded_data = False\n",
        "\n",
        "if top_10_classes:\n",
        "  n_classes = 10\n",
        "  miR_data, miR_label, num_miR_label = top_10_dataset(miR_data, miR_label)\n",
        "else:\n",
        "  n_classes = np.unique(miR_label).size\n",
        "\n",
        "print(miR_data.shape)\n",
        "num_inputs, train_loader, test_loader = build_dataloader(miR_data, num_miR_label, padded_data, batch_size=128)\n",
        "\n",
        "grad = surrogate.fast_sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CNN architecure definition."
      ],
      "metadata": {
        "id": "WLnGKEPjzUMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_size, w1, wd1, h1, w2, wd2, h2, w3, wd3, h3, w4):\n",
        "        super().__init__()\n",
        "        conv1_out = ((input_size - 1 * (wd1 - 1) -1) + 1)\n",
        "        conv1_out = int(conv1_out)\n",
        "\n",
        "        s1 = (((conv1_out - 1 * (h1 - 1) -1)/h1) + 1)\n",
        "        s1 = int(s1)\n",
        "\n",
        "        conv2_out = ((s1 - 1 * (wd2 - 1)-1) + 1)\n",
        "        conv2_out = int(conv2_out)\n",
        "\n",
        "        s2 = (((conv2_out - 1 * (h2 -1 ) -1) / h2 ) + 1)\n",
        "        s2 = int(s2)\n",
        "\n",
        "        conv3_out = ((s2 - 1 * (wd3 - 1)-1) + 1)\n",
        "        conv3_out = int(conv3_out)\n",
        "\n",
        "        s3_dec = (((conv3_out - 1 * (h3 - 1 ) -1) / h3) + 1)\n",
        "        if s3_dec < 1:\n",
        "            s3 = 1\n",
        "        else:\n",
        "            s3 = math.floor(s3_dec)\n",
        "\n",
        "        if s3 == 0:\n",
        "            s3 = 1\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=w1, kernel_size=wd1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=h1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=w1, out_channels=w2, kernel_size=wd2)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=h2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=w2, out_channels=w3, kernel_size=wd3)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=h3)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(s3*w3, w4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NI0fTjqrr2wQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CNN evaluation\n",
        "Comment/uncomment the line to evaluate CNN_PT_1 or CNN_PT_2."
      ],
      "metadata": {
        "id": "FVnVvbMysMa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with open(\"./best_hyperparams/cnn_params_light.json\", \"r\") as f: #CNN_PT_2\n",
        "with open(\"./best_hyperparams/cnn_params_best.json\", \"r\") as f: #CNN_PT_1\n",
        "  params_cnn = json.load(f)\n",
        "\n",
        "conv_net = ConvNet(num_inputs, w1=params_cnn['w1'], wd1=params_cnn['wd1'], h1=params_cnn['h1'], w2=params_cnn['w2'],\n",
        "                    wd2=params_cnn['wd2'], h2=params_cnn['h2'], w3=params_cnn['w3'], wd3=params_cnn['wd3'], h3=params_cnn['h3'], w4=n_classes)\n",
        "\n",
        "print(f\"CNN nr. of parameters: {count_parameters(conv_net)}\")\n",
        "\n",
        "#state_dict = torch.load(\"./trained_models/cnn_light_trained_model.pt\") #CNN_PT_2\n",
        "state_dict = torch.load(\"./trained_models/cnn_best_acc_trained_model.pt\") #CNN_PT_1\n",
        "conv_net.load_state_dict(state_dict)\n",
        "\n",
        "conv_net.to(device)\n",
        "conv_net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = conv_net(inputs)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"CNN test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "pdVVwjM7r_Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SNN evaluation"
      ],
      "metadata": {
        "id": "VGz136YKu9KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(net, num_steps, data):\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps):\n",
        "      spk_out, mem_out = net(data)\n",
        "      spk_rec.append(spk_out)\n",
        "      mem_rec.append(mem_out)\n",
        "\n",
        "  return torch.stack(spk_rec), torch.stack(mem_rec)\n",
        "\n",
        "def test_accuracy(train_loader, net, num_steps, device):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    net.eval()\n",
        "\n",
        "    train_loader = iter(train_loader)\n",
        "    for data, targets in train_loader:\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        spk_rec, _ = forward_pass(net, num_steps, data)\n",
        "\n",
        "\n",
        "        acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "  return acc/total"
      ],
      "metadata": {
        "id": "WnoYcZjXvHk8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Network and hyperparameters definition.\n",
        "Select one of these two following cell to define SNN_PT_1 (first cell) or SNN_PT_2 (second cell). Comment/uncomment some lines to choice the set of hyperparams (best accuracy set or tradeoff accuracy/num_step set)."
      ],
      "metadata": {
        "id": "h1xZJN5ix8I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./best_hyperparams/cnn_params_best.json\", \"r\") as f:\n",
        "  params_cnn = json.load(f)\n",
        "\n",
        "s1, s2, s3 = get_cnn_dimension(num_inputs, params_cnn)\n",
        "\n",
        "#with open(\"./best_hyperparams/snn_params_SNN_PT_1_ce_best.json\", \"r\") as f:\n",
        "with open(\"./best_hyperparams/snn_params_SNN_PT_1_ce_tradeoff.json\", \"r\") as f:\n",
        "  params_snn = json.load(f)\n",
        "\n",
        "#state_dict = torch.load(\"./trained_models/SNN_PT_1_best.pt\")\n",
        "state_dict = torch.load(\"./trained_models/SNN_PT_1_tradeoff.pt\")"
      ],
      "metadata": {
        "id": "LfYz20LJvp00"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./best_hyperparams/cnn_params_light.json\", \"r\") as f:\n",
        "  params_cnn = json.load(f)\n",
        "\n",
        "s1, s2, s3 = get_cnn_dimension(num_inputs, params_cnn)\n",
        "\n",
        "#with open(\"./best_hyperparams/snn_params_SNN_PT_2_ce_tradeoff.json\", \"r\") as f:\n",
        "with open(\"./best_hyperparams/snn_params_SNN_PT_2_ce_best.json\", \"r\") as f:\n",
        "  params_snn = json.load(f)\n",
        "\n",
        "#state_dict = torch.load(\"./trained_models/SNN_PT_2_tradeoff.pt\")\n",
        "state_dict = torch.load(\"./trained_models/SNN_PT_2_best.pt\")"
      ],
      "metadata": {
        "id": "6FQUBZtsxE81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snn_model = nn.Sequential(\n",
        "  nn.Conv1d(in_channels=1, out_channels=params_cnn['w1'], kernel_size=params_cnn['wd1']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h1']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s1),\n",
        "  nn.Conv1d(in_channels=params_cnn['w1'], out_channels=params_cnn['w2'], kernel_size=params_cnn['wd2']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h2']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s2),\n",
        "  nn.Conv1d(in_channels=params_cnn['w2'], out_channels=params_cnn['w3'], kernel_size=params_cnn['wd3']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h3']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s3),\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(s3*params_cnn['w3'], n_classes),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, n_classes, output=True)\n",
        ")\n",
        "\n",
        "print(f\"SNN nr. of parameters: {count_parameters(snn_model)}\")\n",
        "\n",
        "snn_model.load_state_dict(state_dict)\n",
        "snn_model.to(device)\n",
        "\n",
        "accuracy = test_accuracy(test_loader, snn_model, params_snn['num_steps'], device)\n",
        "print(f\"SNN test Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "OMRcHhsEw-mM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}