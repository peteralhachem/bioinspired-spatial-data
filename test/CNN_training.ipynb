{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Non-spiking CNN architecture: definition and training."
      ],
      "metadata": {
        "id": "p4cVKRYcjpqJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZpUrcYSUXSf",
        "outputId": "3fea53de-d666-4f52-d4c9-7b77da8d3118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mla-prj-23-mla-prj-12-gu1'...\n",
            "remote: Enumerating objects: 681, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 681 (delta 8), reused 6 (delta 6), pack-reused 672\u001b[K\n",
            "Receiving objects: 100% (681/681), 53.62 MiB | 25.46 MiB/s, done.\n",
            "Resolving deltas: 100% (401/401), done.\n",
            "Updating files: 100% (95/95), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if \"mla-prj-23-mla-prj-12-gu1\" not in os.listdir(\"./\"):\n",
        "  !git clone https://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@github.com/MLinApp-polito/mla-prj-23-mla-prj-12-gu1.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mla-prj-23-mla-prj-12-gu1/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spX-cPKmaBef",
        "outputId": "80d1c399-b800-44ff-e9b6-fbddbbe38d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mla-prj-23-mla-prj-12-gu1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import csv\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pprint\n",
        "import tensorflow.keras as tk\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import json"
      ],
      "metadata": {
        "id": "O3VRI3F3Wa2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Utilities function for data loading and processing."
      ],
      "metadata": {
        "id": "SbXWy2vkVaCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pad_data(data):\n",
        "  miR_data = data\n",
        "  c_int = math.ceil(np.sqrt(len(miR_data[0])))\n",
        "  pad = c_int ** 2 - len(miR_data[0])\n",
        "  pad_width = (0, pad)\n",
        "\n",
        "  padded_miR_data = np.zeros((miR_data.shape[0], miR_data.shape[1] + pad_width[1]))\n",
        "\n",
        "  for i in range(len(miR_data)):\n",
        "    padded_miR_data[i] = np.pad(miR_data[i], pad_width, mode='constant')\n",
        "\n",
        "  # reshape shape[1] into (c_int, c_int)\n",
        "\n",
        "  dim = int(np.sqrt(len(padded_miR_data[0])))\n",
        "  padded_miR_data = padded_miR_data.reshape((padded_miR_data.shape[0],1, dim, dim))\n",
        "\n",
        "  return padded_miR_data\n",
        "\n",
        "def build_dataloader(miR_data, num_miR_label, padded_data, batch_size=404):\n",
        "\n",
        "    if padded_data:\n",
        "        miR_data = add_pad_data(miR_data)\n",
        "\n",
        "    train_data, val_data, train_label, val_label = train_test_split(miR_data, num_miR_label, test_size=0.20, random_state=42)\n",
        "\n",
        "    miR_train = torch.Tensor(train_data)\n",
        "    miR_train = miR_train.unsqueeze(1)\n",
        "    miR_train_label = torch.LongTensor(train_label)\n",
        "    miR_dataset_train = TensorDataset(miR_train, miR_train_label)\n",
        "\n",
        "    miR_val = torch.Tensor(val_data)\n",
        "    miR_val = miR_val.unsqueeze(1)\n",
        "    miR_val_label = torch.LongTensor(val_label)\n",
        "    miR_dataset_val = TensorDataset(miR_val, miR_val_label)\n",
        "\n",
        "    train_loader = DataLoader(miR_dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(miR_dataset_val, batch_size=batch_size)\n",
        "\n",
        "    if padded_data:\n",
        "        num_inputs = train_data.shape[2] ** 2\n",
        "    else:\n",
        "        num_inputs = train_data.shape[1]\n",
        "\n",
        "    return num_inputs, train_loader, test_loader\n",
        "\n",
        "def normalize(data, method='zscore'):\n",
        "    if method == \"zscore\":\n",
        "        return scipy.stats.zscore(data, axis=1)\n",
        "\n",
        "    # log2 normalization\n",
        "    elif method==\"log2\":\n",
        "        data = data + abs(np.min(data)) + 0.001\n",
        "        return np.log2(data)\n",
        "\n",
        "    # normalization between [0, 255]\n",
        "    else:\n",
        "       return (data - np.min(data)) / (np.max(data) - np.min(data)) * 255\n",
        "\n",
        "def extract_label(file_name, verbose=False):\n",
        "    data = {}\n",
        "    label = []\n",
        "    with open(file_name, \"r\") as fin:\n",
        "        reader = csv.reader(fin, delimiter=',')\n",
        "        first = True\n",
        "        for row in reader:\n",
        "            lbl = row[2]\n",
        "            if first or \"TARGET\" in lbl:\n",
        "                first = False\n",
        "                continue\n",
        "            lbl = lbl.replace(\"TCGA-\",\"\")\n",
        "\n",
        "            label.append(lbl)\n",
        "            if lbl in data.keys():\n",
        "                data[lbl] += 1\n",
        "            else:\n",
        "                data[lbl] = 1\n",
        "    if verbose:\n",
        "        print(f\"Number of classes in the dataset = {len(data)}\")\n",
        "        pprint.pprint(data, indent=4)\n",
        "\n",
        "    return label\n",
        "\n",
        "def create_dictionary(labels):\n",
        "    dictionary = {}\n",
        "    class_names = np.unique(labels)\n",
        "    for i, name in enumerate(class_names):\n",
        "        dictionary[name] = i\n",
        "    return dictionary\n",
        "\n",
        "def label_processing(labels):\n",
        "    new_miRna_label = []\n",
        "    dictionary = create_dictionary(labels)\n",
        "    for i in labels:\n",
        "        new_miRna_label.append(dictionary[i])\n",
        "    return new_miRna_label\n",
        "\n",
        "def top_10_dataset(miR_data, miR_label):\n",
        "  occ = dict({k: 0 for k in set(miR_label)})\n",
        "\n",
        "  for i in range(len(miR_label)):\n",
        "    occ[miR_label[i]] += 1\n",
        "\n",
        "  top_10_class = sorted(occ, key=occ.get,reverse=True)[:10]\n",
        "\n",
        "  list_top_10_train = []\n",
        "  list_top_10_labels = []\n",
        "\n",
        "  for i in range(len(miR_label)):\n",
        "    if miR_label[i] in top_10_class:\n",
        "      list_top_10_labels.append(miR_label[i])\n",
        "\n",
        "  for i in range(miR_data.shape[0]):\n",
        "    if miR_label[i] in top_10_class:\n",
        "      list_top_10_train.append(miR_data[i])\n",
        "\n",
        "  miR_data_reduced = np.stack(list_top_10_train, axis=0)\n",
        "  miR_label_reduced = list_top_10_labels\n",
        "\n",
        "  num_miR_label_reduced = label_processing(miR_label_reduced)\n",
        "\n",
        "  return miR_data_reduced, miR_label_reduced, num_miR_label_reduced\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "7X5J1xJ1U9jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Non-spiking CNN architecture."
      ],
      "metadata": {
        "id": "88-SbLWPXJ0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_size, w1, wd1, h1, w2, wd2, h2, w3, wd3, h3, w4):\n",
        "        super().__init__()\n",
        "        conv1_out = ((input_size - 1 * (wd1 - 1) -1) + 1)\n",
        "        conv1_out = int(conv1_out)\n",
        "\n",
        "        s1 = (((conv1_out - 1 * (h1 - 1) -1)/h1) + 1)\n",
        "        s1 = int(s1)\n",
        "\n",
        "        conv2_out = ((s1 - 1 * (wd2 - 1)-1) + 1)\n",
        "        conv2_out = int(conv2_out)\n",
        "\n",
        "        s2 = (((conv2_out - 1 * (h2 -1 ) -1) / h2 ) + 1)\n",
        "        s2 = int(s2)\n",
        "\n",
        "        conv3_out = ((s2 - 1 * (wd3 - 1)-1) + 1)\n",
        "        conv3_out = int(conv3_out)\n",
        "\n",
        "        s3_dec = (((conv3_out - 1 * (h3 - 1 ) -1) / h3) + 1)\n",
        "        if s3_dec < 1:\n",
        "            s3 = 1\n",
        "        else:\n",
        "            s3 = math.floor(s3_dec)\n",
        "\n",
        "        if s3 == 0:\n",
        "            s3 = 1\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=w1, kernel_size=wd1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=h1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=w1, out_channels=w2, kernel_size=wd2)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=h2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=w2, out_channels=w3, kernel_size=wd3)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=h3)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(s3*w3, w4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "o7VvBxiHXH7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainCNN(model, device, train_dataloader, test_dataloader, num_epochs = 173):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {running_loss / len(train_dataloader)}\")\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in test_dataloader:\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                predicted = torch.argmax(outputs, dim=1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "FuXobQTzVgd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading data and training the network\n"
      ],
      "metadata": {
        "id": "af9JJusqXoLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Running on device \", device)\n",
        "miR_label = extract_label(\"./dataset/tcga_mir_label.csv\")\n",
        "miR_data = np.genfromtxt('./dataset/tcga_mir_rpm.csv', delimiter=',')[1:,0:-1]\n",
        "number_to_delete = abs(len(miR_label) - miR_data.shape[0])\n",
        "miR_data = miR_data[number_to_delete:,:]\n",
        "# Convert labels in number\n",
        "num_miR_label = label_processing(miR_label)\n",
        "\n",
        "# Z-score\n",
        "miR_data = normalize(miR_data)\n",
        "\n",
        "assert np.isnan(miR_data).sum() == 0\n",
        "\n",
        "#---Number of classes---#\n",
        "top_10_classes = True\n",
        "padded_data = False\n",
        "\n",
        "if top_10_classes:\n",
        "    n_classes = 10\n",
        "    miR_data, miR_label, num_miR_label = top_10_dataset(miR_data, miR_label)\n",
        "else:\n",
        "    n_classes = np.unique(miR_label).size\n",
        "\n",
        "num_inputs, train_loader, test_loader = build_dataloader(miR_data, num_miR_label, padded_data, batch_size=128)"
      ],
      "metadata": {
        "id": "Y5laRYYHWMBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cells define the set of the hypeparameters found by NNI experiments for this architecture:\n",
        "- The first corresponds to the one that build a network that gives us the best test accuracy so far, called network CNN_PT_1 in the project report\n",
        "- The second represents the one that build a network with the best compromise in terms of test accuracy, number of parameters, and training time, called network CNN_PT_2 in the project report\n",
        "\n",
        "Run one of these two cells to define the network hyperparameters and training the CNN."
      ],
      "metadata": {
        "id": "DDV-MaFucgZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN_PT_1 hyperparams\n",
        "with open(\"./best_hyperparams/cnn_params_best.json\", \"r\") as f:\n",
        "  params_cnn = json.load(f)\n",
        "\n",
        "conv_net = ConvNet(num_inputs, w1=params_cnn['w1'], wd1=params_cnn['wd1'], h1=params_cnn['h1'], w2=params_cnn['w2'],\n",
        "                    wd2=params_cnn['wd2'], h2=params_cnn['h2'], w3=params_cnn['w3'], wd3=params_cnn['wd3'], h3=params_cnn['h3'], w4=n_classes)\n",
        "\n",
        "print(f\"Network nr. of parameters: {count_parameters(conv_net)}\")\n",
        "trainCNN(conv_net, device=device, train_dataloader=train_loader, test_dataloader=test_loader, num_epochs=50)"
      ],
      "metadata": {
        "id": "NWm2xS_FccCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN_PT_2 hyperparams\n",
        "with open(\"./best_hyperparams/cnn_params_light.json\", \"r\") as f:\n",
        "  params_cnn = json.load(f)\n",
        "\n",
        "conv_net = ConvNet(num_inputs, w1=params_cnn['w1'], wd1=params_cnn['wd1'], h1=params_cnn['h1'], w2=params_cnn['w2'],\n",
        "                    wd2=params_cnn['wd2'], h2=params_cnn['h2'], w3=params_cnn['w3'], wd3=params_cnn['wd3'], h3=params_cnn['h3'], w4=n_classes)\n",
        "\n",
        "print(f\"Network nr. of parameters: {count_parameters(conv_net)}\")\n",
        "trainCNN(conv_net, device=device, train_dataloader=train_loader, test_dataloader=test_loader, num_epochs=50)"
      ],
      "metadata": {
        "id": "XVEdYauUcoLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}