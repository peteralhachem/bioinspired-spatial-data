{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDTq4PEbjsHc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if \"mla-prj-23-mla-prj-12-gu1\" not in os.listdir(\"./\"):\n",
        "  !git clone https://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@github.com/MLinApp-polito/mla-prj-23-mla-prj-12-gu1.git\n",
        "\n",
        "!pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mla-prj-23-mla-prj-12-gu1/"
      ],
      "metadata": {
        "id": "95KOTZBpkKAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import csv\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import snntorch as snn\n",
        "from snntorch import utils, surrogate\n",
        "import snntorch.functional as SF\n",
        "import json"
      ],
      "metadata": {
        "id": "mZPb6Y_3rrvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Utilities function for network definition and training"
      ],
      "metadata": {
        "id": "PWP2iOBJkMFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TBPTT(\n",
        "    net,\n",
        "    dataloader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    num_steps=False,  # only specified if time-static\n",
        "    time_var=True,  # specifies if data is time_varying\n",
        "    time_first=True,\n",
        "    regularization=False,\n",
        "    device=\"cpu\",\n",
        "    K=1,\n",
        "):\n",
        "    \"\"\"Truncated backpropagation through time. LIF layers require parameter\n",
        "    ``init_hidden = True``.\n",
        "    Weight updates are performed every ``K`` time steps.\n",
        "\n",
        "    Example::\n",
        "\n",
        "        import snntorch as snn\n",
        "        import snntorch.functional as SF\n",
        "        from snntorch import utils\n",
        "        from snntorch import backprop\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "\n",
        "        lif1 = snn.Leaky(beta=0.9, init_hidden=True)\n",
        "        lif2 = snn.Leaky(beta=0.9, init_hidden=True, output=True)\n",
        "\n",
        "        net = nn.Sequential(nn.Flatten(),\n",
        "                            nn.Linear(784,500),\n",
        "                            lif1,\n",
        "                            nn.Linear(500, 10),\n",
        "                            lif2).to(device)\n",
        "\n",
        "        device = torch.device(\"cuda\") if torch.cuda.is_available() else\n",
        "        torch.device(\"cpu\")\n",
        "        num_steps = 100\n",
        "\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=5e-4,\n",
        "        betas=(0.9, 0.999))\n",
        "        loss_fn = SF.mse_count_loss()\n",
        "        reg_fn = SF.l1_rate_sparsity()\n",
        "\n",
        "        # train_loader is of type torch.utils.data.DataLoader\n",
        "        # if input data is time-static, set time_var=False, and specify\n",
        "        # num_steps.\n",
        "        # if input data is time-varying, set time_var=True and do not\n",
        "        # specify num_steps.\n",
        "        # backprop is automatically applied every K=40 time steps\n",
        "\n",
        "        for epoch in range(5):\n",
        "            loss = backprop.RTRL(net, train_loader, optimizer=optimizer,\n",
        "            criterion=loss_fn, num_steps=num_steps, time_var=False,\n",
        "            regularization=reg_fn, device=device, K=40)\n",
        "\n",
        "\n",
        "    :param net: Network model (either wrapped in Sequential container or as a\n",
        "        class)\n",
        "    :type net: torch.nn.modules.container.Sequential\n",
        "\n",
        "    :param dataloader: DataLoader containing data and targets\n",
        "    :type dataloader: torch.utils.data.DataLoader\n",
        "\n",
        "    :param optimizer: Optimizer used, e.g., torch.optim.adam.Adam\n",
        "    :type optimizer: torch.optim\n",
        "\n",
        "    :param criterion: Loss criterion from snntorch.functional, e.g.,\n",
        "        snn.functional.mse_count_loss()\n",
        "    :type criterion: snn.functional.LossFunctions\n",
        "\n",
        "    :param num_steps: Number of time steps. Does not need to be\n",
        "        specified if ``time_var=True``.\n",
        "    :type num_steps: int, optional\n",
        "\n",
        "    :param time_var: Set to ``True`` if input data is time-varying\n",
        "        [T x B x dims]. Otherwise, set to false if input data is time-static\n",
        "        [B x dims], defaults to ``True``\n",
        "    :type time_var: Bool, optional\n",
        "\n",
        "    :param time_first: Set to ``False`` if first dimension of data is not\n",
        "        time [B x T x dims] AND must also be permuted to [T x B x dims],\n",
        "        defaults to ``True``\n",
        "    :type time_first: Bool, optional\n",
        "\n",
        "    :param regularization: Option to add a regularization term to the loss\n",
        "        function\n",
        "    :type regularization: snn.functional regularization function, optional\n",
        "\n",
        "    :param device: Specify either \"cuda\" or \"cpu\", defaults to \"cpu\"\n",
        "    :type device: string, optional\n",
        "\n",
        "    :param K: Number of time steps to process per weight update, defaults\n",
        "        to ``1``\n",
        "    :type K: int, optional\n",
        "\n",
        "    :return: return average loss for one epoch\n",
        "    :rtype: torch.Tensor\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if num_steps and time_var:\n",
        "        raise ValueError(\n",
        "            \"``num_steps`` should not be specified if time_var is ``True``. \"\n",
        "            \"When using time-varying input data, the size of the time-first \"\n",
        "            \"dimension of each batch is automatically used as ``num_steps``.\"\n",
        "        )\n",
        "\n",
        "    if num_steps is False and time_var is False:\n",
        "        raise ValueError(\n",
        "            \"``num_steps`` must be specified if ``time_var`` is ``False``. \"\n",
        "            \"When using time-static input data, ``num_steps`` must be \"\n",
        "            \"passed in.\"\n",
        "        )\n",
        "\n",
        "    if num_steps and K > num_steps:\n",
        "        raise ValueError(\"``K`` must be less than or equal to ``num_steps``.\")\n",
        "\n",
        "    if time_var is False and time_first is False:\n",
        "        raise ValueError(\n",
        "            \"``time_first`` should not be specified if data is not \"\n",
        "            \"time-varying, i.e., ``time_var`` is ``False``.\"\n",
        "        )\n",
        "\n",
        "    # triggers global variables is_lapicque etc for neurons_dict\n",
        "    # redo reset in training loop\n",
        "    utils.reset(net=net)\n",
        "\n",
        "    neurons_dict = {\n",
        "        utils.is_lapicque: snn.Lapicque,\n",
        "        utils.is_leaky: snn.Leaky,\n",
        "        utils.is_synaptic: snn.Synaptic,\n",
        "        utils.is_alpha: snn.Alpha,\n",
        "        utils.is_rleaky: snn.RLeaky,\n",
        "        utils.is_rsynaptic: snn.RSynaptic,\n",
        "        utils.is_sconv2dlstm: snn.SConv2dLSTM,\n",
        "        utils.is_slstm: snn.SLSTM,\n",
        "    }\n",
        "\n",
        "    # element 1: if true: spk, if false, mem\n",
        "    # element 2: if true: time_varying_targets\n",
        "    criterion_dict = {\n",
        "        \"mse_membrane_loss\": [\n",
        "            False,\n",
        "            True,\n",
        "        ],  # if time_var_target is true, need a flag to let mse_mem_loss\n",
        "        # know when to re-start iterating targets from\n",
        "        \"ce_max_membrane_loss\": [False, False],\n",
        "        \"ce_rate_loss\": [True, False],\n",
        "        \"ce_count_loss\": [True, False],\n",
        "        \"mse_count_loss\": [True, False],\n",
        "        \"ce_latency_loss\": [True, False],\n",
        "        \"mse_temporal_loss\": [True, False],\n",
        "        \"ce_temporal_loss\": [True, False],\n",
        "    }  # note: when using mse_count_loss, the target spike-count should be\n",
        "    # for a truncated time, not for the full time\n",
        "\n",
        "    reg_dict = {\"l1_rate_sparsity\": True}\n",
        "\n",
        "    # acc_dict = {\n",
        "    #     SF.accuracy_rate : [False, False, False, True]\n",
        "    # }\n",
        "\n",
        "    time_var_targets = False\n",
        "    counter = len(criterion_dict)\n",
        "    for criterion_key in criterion_dict:\n",
        "        if criterion_key == criterion.__name__:\n",
        "            loss_spk, time_var_targets = criterion_dict[\n",
        "                criterion_key\n",
        "            ]  # m: mem, s: spk // s: every step, e: end\n",
        "            if time_var_targets:\n",
        "                time_var_targets = criterion.time_var_targets  # check this\n",
        "        counter -= 1\n",
        "    if counter:  # fix the print statement\n",
        "        raise TypeError(\n",
        "            \"``criterion`` must be one of the loss functions in \"\n",
        "            \"``snntorch.functional``: e.g., 'mse_membrane_loss', \"\n",
        "            \"'ce_max_membrane_loss', 'ce_rate_loss' etc.\"\n",
        "        )\n",
        "\n",
        "    if regularization:\n",
        "        for reg_item in reg_dict:\n",
        "            if reg_item == regularization.__name__:\n",
        "                # m: mem, s: spk // s: every step, e: end\n",
        "                reg_spk = reg_dict[reg_item]\n",
        "\n",
        "    num_return = utils._final_layer_check(net)  # number of outputs\n",
        "\n",
        "    step_trunc = 0  # ranges from 0 to K, resetting every K time steps\n",
        "    K_count = 0\n",
        "    loss_trunc = 0  # reset every K time steps\n",
        "    loss_avg = 0\n",
        "    iter_count = 0\n",
        "\n",
        "    mem_rec_trunc = []\n",
        "    spk_rec_trunc = []\n",
        "\n",
        "    net = net.to(device)\n",
        "\n",
        "    data_iterator = iter(dataloader)\n",
        "    for data, targets in data_iterator:\n",
        "        iter_count += 1\n",
        "        net.train()\n",
        "        data = data.to(device, dtype=torch.float)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "        if time_var:\n",
        "            if time_first:\n",
        "                num_steps = data.size(0)\n",
        "            else:\n",
        "                num_steps = data.size(1)\n",
        "\n",
        "            if K is False:\n",
        "                K_flag = K\n",
        "            if K_flag is False:\n",
        "                K = num_steps\n",
        "\n",
        "        utils.reset(net)\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            if num_return == 2:\n",
        "                if time_var:\n",
        "                    if time_first:\n",
        "                        spk, mem = net(data[step])\n",
        "                    else:\n",
        "                        spk, mem = net(data.transpose(1, 0)[step])\n",
        "                else:\n",
        "                    spk, mem = net(data)\n",
        "\n",
        "            elif num_return == 3:\n",
        "                if time_var:\n",
        "                    if time_first:\n",
        "                        spk, _, mem = net(data[step])\n",
        "                    else:\n",
        "                        spk, _, mem = net(data.transpose(1, 0)[step])\n",
        "                else:\n",
        "                    spk, _, mem = net(data)\n",
        "\n",
        "            elif num_return == 4:\n",
        "                if time_var:\n",
        "                    if time_first:\n",
        "                        spk, _, _, mem = net(data[step])\n",
        "                    else:\n",
        "                        spk, _, _, mem = net(data.transpose(1, 0)[step])\n",
        "                else:\n",
        "                    spk, _, _, mem = net(data)\n",
        "\n",
        "            # else:  # assume not an snn.Layer returning 1 val\n",
        "            #     if time_var:\n",
        "            #         spk = net(data[step])\n",
        "            #     else:\n",
        "            #         spk = net(data)\n",
        "            #     spk_rec.append(spk)\n",
        "\n",
        "            spk_rec_trunc.append(spk)\n",
        "            mem_rec_trunc.append(mem)\n",
        "\n",
        "            step_trunc += 1\n",
        "            if step_trunc == K:\n",
        "                # spk_rec += spk_rec_trunc # test\n",
        "                # mem_rec += mem_rec_trunc # test\n",
        "\n",
        "                spk_rec_trunc = torch.stack(spk_rec_trunc, dim=0)\n",
        "                mem_rec_trunc = torch.stack(mem_rec_trunc, dim=0)\n",
        "\n",
        "                # loss_spk is True if input to criterion is spk;\n",
        "                # reg_spk is True if input to reg is spk\n",
        "\n",
        "                # catch case for time_varying_targets?\n",
        "                if time_var_targets:\n",
        "                    if loss_spk:\n",
        "                        loss = criterion(\n",
        "                            spk_rec_trunc,\n",
        "                            targets[int(K_count * K) : int((K_count + 1) * K)],\n",
        "                        )\n",
        "                    else:\n",
        "                        loss = criterion(\n",
        "                            mem_rec_trunc,\n",
        "                            targets[int(K_count * K) : int((K_count + 1) * K)],\n",
        "                        )\n",
        "                else:\n",
        "                    if loss_spk:\n",
        "                        loss = criterion(spk_rec_trunc, targets.long())\n",
        "                    else:\n",
        "                        loss = criterion(mem_rec_trunc, targets.long())\n",
        "\n",
        "                if regularization:\n",
        "                    if reg_spk:\n",
        "                        loss += regularization(spk_rec_trunc)\n",
        "                    else:\n",
        "                        loss += regularization(mem_rec_trunc)\n",
        "\n",
        "                loss_trunc += loss\n",
        "                loss_avg += loss / (num_steps / K)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss_trunc.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                for neuron in neurons_dict:\n",
        "                    if neuron:\n",
        "                        neurons_dict[neuron].detach_hidden()\n",
        "                        # detach_hidden --> _reset_hidden\n",
        "\n",
        "                K_count += 1\n",
        "                step_trunc = 0\n",
        "                loss_trunc = 0\n",
        "                spk_rec_trunc = []\n",
        "                mem_rec_trunc = []\n",
        "\n",
        "        if (step == num_steps - 1) and (num_steps % K):\n",
        "            spk_rec_trunc = torch.stack(spk_rec_trunc, dim=0)\n",
        "            mem_rec_trunc = torch.stack(mem_rec_trunc, dim=0)\n",
        "\n",
        "            if time_var_targets:\n",
        "                idx1 = K_count * K\n",
        "                idx2 = K_count * K + num_steps % K\n",
        "                if loss_spk:\n",
        "                    loss = criterion(\n",
        "                        spk_rec_trunc,\n",
        "                        targets[int(idx1) : int(idx2)],\n",
        "                    )\n",
        "                else:\n",
        "                    loss = criterion(\n",
        "                        mem_rec_trunc,\n",
        "                        targets[int(idx1) : int(idx2)],\n",
        "                    )\n",
        "            else:\n",
        "                if loss_spk:\n",
        "                    loss = criterion(spk_rec_trunc, targets)\n",
        "                else:\n",
        "                    loss = criterion(mem_rec_trunc, targets)\n",
        "\n",
        "            if regularization:\n",
        "                if reg_spk:\n",
        "                    loss += regularization(spk_rec_trunc)\n",
        "                else:\n",
        "                    loss += regularization(mem_rec_trunc)\n",
        "\n",
        "            loss_trunc += loss\n",
        "            loss_avg += loss / int(num_steps % K)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_trunc.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            K_count = 0\n",
        "            step_trunc = 0\n",
        "            loss_trunc = 0\n",
        "            spk_rec_trunc = []\n",
        "            mem_rec_trunc = []\n",
        "\n",
        "            for neuron in neurons_dict:\n",
        "                if neuron:\n",
        "                    neurons_dict[neuron].detach_hidden()\n",
        "\n",
        "    return loss_avg / iter_count  # , spk_rec, mem_rec\n",
        "\n",
        "def BPTT(\n",
        "    net,\n",
        "    dataloader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    num_steps=False,\n",
        "    time_var=True,\n",
        "    time_first=True,\n",
        "    regularization=False,\n",
        "    device=\"cpu\",\n",
        "):\n",
        "    \"\"\"Backpropagation through time. LIF layers require parameter\n",
        "    ``init_hidden = True``.\n",
        "    A forward pass is applied for each time step while the loss accumulates.\n",
        "    The backward pass and parameter update is only applied at the end of\n",
        "    each time step sequence.\n",
        "    BPTT is equivalent to TBPTT for the case where ``num_steps = K``.\n",
        "\n",
        "    Example::\n",
        "\n",
        "        import snntorch as snn\n",
        "        import snntorch.functional as SF\n",
        "        from snntorch import utils\n",
        "        from snntorch import backprop\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "\n",
        "        lif1 = snn.Leaky(beta=0.9, init_hidden=True)\n",
        "        lif2 = snn.Leaky(beta=0.9, init_hidden=True, output=True)\n",
        "\n",
        "        net = nn.Sequential(nn.Flatten(),\n",
        "                            nn.Linear(784,500),\n",
        "                            lif1,\n",
        "                            nn.Linear(500, 10),\n",
        "                            lif2).to(device)\n",
        "\n",
        "        device = torch.device(\"cuda\") if torch.cuda.is_available() else\n",
        "        torch.device(\"cpu\")\n",
        "        num_steps = 100\n",
        "\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=5e-4,\n",
        "        betas=(0.9, 0.999))\n",
        "        loss_fn = SF.mse_count_loss()\n",
        "        reg_fn = SF.l1_rate_sparsity()\n",
        "\n",
        "\n",
        "        # train_loader is of type torch.utils.data.DataLoader\n",
        "        # if input data is time-static, set time_var=False, and specify\n",
        "        # num_steps.\n",
        "        # if input data is time-varying, set time_var=True and do not\n",
        "        # specify num_steps.\n",
        "\n",
        "        for epoch in range(5):\n",
        "            loss = backprop.RTRL(net, train_loader, optimizer=optimizer,\n",
        "            criterion=loss_fn, num_steps=num_steps, time_var=False,\n",
        "            regularization=reg_fn, device=device)\n",
        "\n",
        "\n",
        "    :param net: Network model (either wrapped in Sequential container or as\n",
        "        a class)\n",
        "    :type net: torch.nn.modules.container.Sequential\n",
        "\n",
        "    :param dataloader: DataLoader containing data and targets\n",
        "    :type dataloader: torch.utils.data.DataLoader\n",
        "\n",
        "    :param optimizer: Optimizer used, e.g., torch.optim.adam.Adam\n",
        "    :type optimizer: torch.optim\n",
        "\n",
        "    :param criterion: Loss criterion from snntorch.functional, e.g.,\n",
        "        snn.functional.mse_count_loss()\n",
        "    :type criterion: snn.functional.LossFunctions\n",
        "\n",
        "    :param num_steps: Number of time steps. Does not need to be specified if\n",
        "        ``time_var=True``.\n",
        "    :type num_steps: int, optional\n",
        "\n",
        "    :param time_var: Set to ``True`` if input data is time-varying\n",
        "        [T x B x dims]. Otherwise, set to false if input data is time-static\n",
        "        [B x dims], defaults to ``True``\n",
        "    :type time_var: Bool, optional\n",
        "\n",
        "    :param time_first: Set to ``False`` if first dimension of data is not\n",
        "        time [B x T x dims] AND must also be permuted to [T x B x dims],\n",
        "        defaults to ``True``\n",
        "    :type time_first: Bool, optional\n",
        "\n",
        "    :param regularization: Option to add a regularization term to the loss\n",
        "        function\n",
        "    :type regularization: snn.functional regularization function, optional\n",
        "\n",
        "    :param device: Specify either \"cuda\" or \"cpu\", defaults to \"cpu\"\n",
        "    :type device: string, optional\n",
        "\n",
        "    :return: return average loss for one epoch\n",
        "    :rtype: torch.Tensor\n",
        "    \"\"\"\n",
        "\n",
        "    #  Net requires hidden instance variables rather than global instance\n",
        "    #  variables for TBPTT\n",
        "    return TBPTT(\n",
        "        net,\n",
        "        dataloader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        num_steps,\n",
        "        time_var,\n",
        "        time_first,\n",
        "        regularization,\n",
        "        device,\n",
        "        K=num_steps,\n",
        "    )\n",
        "\n",
        "def test_accuracy(data_loader, net, device, population_code=False, num_classes=False):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    net.eval()\n",
        "\n",
        "    data_loader = iter(data_loader)\n",
        "    for data, targets in data_loader:\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "      utils.reset(net)\n",
        "      spk_rec, _ = net(data)\n",
        "\n",
        "      if population_code:\n",
        "        acc += SF.accuracy_rate(spk_rec.unsqueeze(0), targets, population_code=True, num_classes=num_classes) * spk_rec.size(1)\n",
        "      else:\n",
        "        acc += SF.accuracy_rate(spk_rec.unsqueeze(0), targets) * spk_rec.size(1)\n",
        "\n",
        "      total += spk_rec.size(1)\n",
        "\n",
        "  return acc/total\n",
        "\n",
        "def trainSNN(net, device, learning_rate, num_epochs, num_steps, population_coding, n_classes, train_loader, test_loader):\n",
        "    curr_acc = -np.inf\n",
        "    net = net.to(device)\n",
        "    print(f\"Net number of parameters: {count_parameters(net)}\")\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "    loss_fn = SF.mse_count_loss(correct_rate=1.0, incorrect_rate=0.0, population_code=population_coding, num_classes=n_classes)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        avg_loss = BPTT(net, train_loader, num_steps=num_steps,\n",
        "                    optimizer=optimizer, criterion=loss_fn, time_var=False, device=device)\n",
        "\n",
        "        epoch_acc = test_accuracy(test_loader, net, device, population_code=population_coding, num_classes=n_classes if population_coding else False)\n",
        "        if epoch_acc >= curr_acc:\n",
        "            curr_acc = epoch_acc\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Test Accuracy: {epoch_acc*100:.2f}%\")\n",
        "    return curr_acc\n",
        "\n",
        "def build_layer(layer_type, beta, grad, num_outputs, output=False):\n",
        "    if layer_type==\"leaky\":\n",
        "        if output==True:\n",
        "            return snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, output=output, threshold=0.4)\n",
        "        else:\n",
        "            return snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, threshold=0.4)\n",
        "\n",
        "    if layer_type==\"lapicque\":\n",
        "        if output==True:\n",
        "            return snn.Lapicque(beta=beta, spike_grad=grad, init_hidden=True, output=output, threshold=0.4)\n",
        "        else:\n",
        "            return snn.Lapicque(beta=beta, spike_grad=grad, init_hidden=True, threshold=0.4)\n",
        "\n",
        "    if layer_type==\"rleaky\":\n",
        "        if output==True:\n",
        "            return snn.RLeaky(beta=beta, spike_grad=grad, init_hidden=True, output=output, linear_features=num_outputs, threshold=0.4)\n",
        "        else:\n",
        "            return snn.RLeaky(beta=beta, spike_grad=grad, init_hidden=True, linear_features=num_outputs, threshold=0.4)\n",
        "\n",
        "def get_cnn_dimension(input_size, params_cnn):\n",
        "    conv1_out = ((input_size - 1 * (params_cnn['wd1'] - 1) -1) + 1)\n",
        "    conv1_out = int(conv1_out)\n",
        "\n",
        "    s1 = (((conv1_out - 1 * (params_cnn['h1'] - 1) -1)/params_cnn['h1']) + 1)\n",
        "    s1 = int(s1)\n",
        "\n",
        "    conv2_out = ((s1 - 1 * (params_cnn['wd2'] - 1)-1) + 1)\n",
        "    conv2_out = int(conv2_out)\n",
        "\n",
        "    s2 = (((conv2_out - 1 * (params_cnn['h2'] -1 ) -1) / params_cnn['h2'] ) + 1)\n",
        "    s2 = int(s2)\n",
        "\n",
        "    conv3_out = ((s2 - 1 * (params_cnn['wd3'] - 1)-1) + 1)\n",
        "    conv3_out = int(conv3_out)\n",
        "\n",
        "    s3_dec = (((conv3_out - 1 * (params_cnn['h3'] - 1 ) -1) / params_cnn['h3']) + 1)\n",
        "    if s3_dec < 1:\n",
        "        s3 = 1\n",
        "    else:\n",
        "        s3 = math.floor(s3_dec)\n",
        "\n",
        "    if s3 == 0:\n",
        "        s3 = 1\n",
        "    return s1, s2, s3"
      ],
      "metadata": {
        "id": "IhWgCVw8kGm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Utilities function for data loading and processing."
      ],
      "metadata": {
        "id": "vSO8yNapkc4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pad_data(data):\n",
        "  miR_data = data\n",
        "  c_int = math.ceil(np.sqrt(len(miR_data[0])))\n",
        "  pad = c_int ** 2 - len(miR_data[0])\n",
        "  pad_width = (0, pad)\n",
        "\n",
        "  padded_miR_data = np.zeros((miR_data.shape[0], miR_data.shape[1] + pad_width[1]))\n",
        "\n",
        "  for i in range(len(miR_data)):\n",
        "    padded_miR_data[i] = np.pad(miR_data[i], pad_width, mode='constant')\n",
        "\n",
        "  # reshape shape[1] into (c_int, c_int)\n",
        "\n",
        "  dim = int(np.sqrt(len(padded_miR_data[0])))\n",
        "  padded_miR_data = padded_miR_data.reshape((padded_miR_data.shape[0],1, dim, dim))\n",
        "\n",
        "  return padded_miR_data\n",
        "\n",
        "def build_dataloader(miR_data, num_miR_label, padded_data, batch_size=404):\n",
        "\n",
        "    if padded_data:\n",
        "        miR_data = add_pad_data(miR_data)\n",
        "\n",
        "    train_data, val_data, train_label, val_label = train_test_split(miR_data, num_miR_label, test_size=0.20, random_state=42)\n",
        "\n",
        "    miR_train = torch.Tensor(train_data)\n",
        "    miR_train = miR_train.unsqueeze(1)\n",
        "    miR_train_label = torch.LongTensor(train_label)\n",
        "    miR_dataset_train = TensorDataset(miR_train, miR_train_label)\n",
        "\n",
        "    miR_val = torch.Tensor(val_data)\n",
        "    miR_val = miR_val.unsqueeze(1)\n",
        "    miR_val_label = torch.LongTensor(val_label)\n",
        "    miR_dataset_val = TensorDataset(miR_val, miR_val_label)\n",
        "\n",
        "    train_loader = DataLoader(miR_dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(miR_dataset_val, batch_size=batch_size)\n",
        "\n",
        "    if padded_data:\n",
        "        num_inputs = train_data.shape[2] ** 2\n",
        "    else:\n",
        "        num_inputs = train_data.shape[1]\n",
        "\n",
        "    return num_inputs, train_loader, test_loader\n",
        "\n",
        "def normalize(data, method='zscore'):\n",
        "    if method == \"zscore\":\n",
        "        return scipy.stats.zscore(data, axis=1)\n",
        "\n",
        "    # log2 normalization\n",
        "    elif method==\"log2\":\n",
        "        data = data + abs(np.min(data)) + 0.001\n",
        "        return np.log2(data)\n",
        "\n",
        "    # normalization between [0, 255]\n",
        "    else:\n",
        "       return (data - np.min(data)) / (np.max(data) - np.min(data)) * 255\n",
        "\n",
        "def extract_label(file_name, verbose=False):\n",
        "    data = {}\n",
        "    label = []\n",
        "    with open(file_name, \"r\") as fin:\n",
        "        reader = csv.reader(fin, delimiter=',')\n",
        "        first = True\n",
        "        for row in reader:\n",
        "            lbl = row[2]\n",
        "            if first or \"TARGET\" in lbl:\n",
        "                first = False\n",
        "                continue\n",
        "            lbl = lbl.replace(\"TCGA-\",\"\")\n",
        "\n",
        "            label.append(lbl)\n",
        "            if lbl in data.keys():\n",
        "                data[lbl] += 1\n",
        "            else:\n",
        "                data[lbl] = 1\n",
        "    if verbose:\n",
        "        print(f\"Number of classes in the dataset = {len(data)}\")\n",
        "        pprint.pprint(data, indent=4)\n",
        "\n",
        "    return label\n",
        "\n",
        "def create_dictionary(labels):\n",
        "    dictionary = {}\n",
        "    class_names = np.unique(labels)\n",
        "    for i, name in enumerate(class_names):\n",
        "        dictionary[name] = i\n",
        "    return dictionary\n",
        "\n",
        "def label_processing(labels):\n",
        "    new_miRna_label = []\n",
        "    dictionary = create_dictionary(labels)\n",
        "    for i in labels:\n",
        "        new_miRna_label.append(dictionary[i])\n",
        "    return new_miRna_label\n",
        "\n",
        "def top_10_dataset(miR_data, miR_label):\n",
        "  occ = dict({k: 0 for k in set(miR_label)})\n",
        "\n",
        "  for i in range(len(miR_label)):\n",
        "    occ[miR_label[i]] += 1\n",
        "\n",
        "  top_10_class = sorted(occ, key=occ.get,reverse=True)[:10]\n",
        "\n",
        "  list_top_10_train = []\n",
        "  list_top_10_labels = []\n",
        "\n",
        "  for i in range(len(miR_label)):\n",
        "    if miR_label[i] in top_10_class:\n",
        "      list_top_10_labels.append(miR_label[i])\n",
        "\n",
        "  for i in range(miR_data.shape[0]):\n",
        "    if miR_label[i] in top_10_class:\n",
        "      list_top_10_train.append(miR_data[i])\n",
        "\n",
        "  miR_data_reduced = np.stack(list_top_10_train, axis=0)\n",
        "  miR_label_reduced = list_top_10_labels\n",
        "\n",
        "  num_miR_label_reduced = label_processing(miR_label_reduced)\n",
        "\n",
        "  return miR_data_reduced, miR_label_reduced, num_miR_label_reduced\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "j-LHy-kSka_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SNN training"
      ],
      "metadata": {
        "id": "c-jnrz5TksiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading"
      ],
      "metadata": {
        "id": "h0eLXsMUrAaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "miR_label = extract_label(\"./dataset/tcga_mir_label.csv\")\n",
        "miR_data = np.genfromtxt('./dataset/tcga_mir_rpm.csv', delimiter=',')[1:,0:-1]\n",
        "number_to_delete = abs(len(miR_label) - miR_data.shape[0])\n",
        "miR_data = miR_data[number_to_delete:,:]\n",
        "\n",
        "# Convert labels in number\n",
        "num_miR_label = label_processing(miR_label)\n",
        "\n",
        "# Z-score\n",
        "miR_data = normalize(miR_data)\n",
        "\n",
        "assert np.isnan(miR_data).sum() == 0\n",
        "\n",
        "#---Number of classes---#\n",
        "top_10_classes = True\n",
        "padded_data = False\n",
        "\n",
        "if top_10_classes:\n",
        "  n_classes = 10\n",
        "  miR_data, miR_label, num_miR_label = top_10_dataset(miR_data, miR_label)\n",
        "else:\n",
        "  n_classes = np.unique(miR_label).size\n",
        "\n",
        "num_inputs, train_loader, test_loader = build_dataloader(miR_data, num_miR_label, padded_data, batch_size=128)\n",
        "\n",
        "grad = surrogate.fast_sigmoid()"
      ],
      "metadata": {
        "id": "S03a5sW-kqn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Network definition and training:\n",
        "Run one of these cell to define and train the spiking network that comes from the non-spiking architecture CNN_PT_1 (first cell) or the one that comes from CNN_PT_2 (second cell)."
      ],
      "metadata": {
        "id": "l55B1J5MrCH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./best_hyperparams/cnn_params_best.json\", \"r\") as f:\n",
        "    params_cnn = json.load(f)\n",
        "\n",
        "s1, s2, s3 = get_cnn_dimension(num_inputs, params_cnn)\n",
        "\n",
        "with open(\"./best_hyperparams/snn_params_mse_best.json\", \"r\") as f:\n",
        "    params_snn = json.load(f)\n",
        "\n",
        "num_outputs = n_classes if params_snn['population_coding']==False else params_snn['neurons_per_classes']*n_classes\n",
        "\n",
        "snn_model = nn.Sequential(\n",
        "  nn.Conv1d(in_channels=1, out_channels=params_cnn['w1'], kernel_size=params_cnn['wd1']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h1']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s1),\n",
        "  nn.Conv1d(in_channels=params_cnn['w1'], out_channels=params_cnn['w2'], kernel_size=params_cnn['wd2']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h2']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s2),\n",
        "  nn.Conv1d(in_channels=params_cnn['w2'], out_channels=params_cnn['w3'], kernel_size=params_cnn['wd3']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h3']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s3),\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(s3*params_cnn['w3'], num_outputs),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, num_outputs, output=True)\n",
        ")\n",
        "\n",
        "trainSNN(snn_model, device , params_snn['learning_rate'], 30, params_snn['num_steps'], params_snn['population_coding'], n_classes, train_loader, test_loader)"
      ],
      "metadata": {
        "id": "isi80DyVqpk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./best_hyperparams/cnn_params_light.json\", \"r\") as f:\n",
        "    params_cnn = json.load(f)\n",
        "\n",
        "s1, s2, s3 = get_cnn_dimension(num_inputs, params_cnn)\n",
        "\n",
        "with open(\"./best_hyperparams/snn_params_mse_light.json\", \"r\") as f:\n",
        "    params_snn = json.load(f)\n",
        "\n",
        "num_outputs = n_classes if params_snn['population_coding']==False else params_snn['neurons_per_classes']*n_classes\n",
        "\n",
        "snn_model = nn.Sequential(\n",
        "  nn.Conv1d(in_channels=1, out_channels=params_cnn['w1'], kernel_size=params_cnn['wd1']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h1']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s1),\n",
        "  nn.Conv1d(in_channels=params_cnn['w1'], out_channels=params_cnn['w2'], kernel_size=params_cnn['wd2']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h2']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s2),\n",
        "  nn.Conv1d(in_channels=params_cnn['w2'], out_channels=params_cnn['w3'], kernel_size=params_cnn['wd3']),\n",
        "  nn.MaxPool1d(kernel_size=params_cnn['h3']),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, s3),\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(s3*params_cnn['w3'], num_outputs),\n",
        "  build_layer(params_snn['neuron_type'], params_snn['beta'], grad, num_outputs, output=True)\n",
        ")\n",
        "\n",
        "trainSNN(snn_model, device , params_snn['learning_rate'], 30, params_snn['num_steps'], params_snn['population_coding'], n_classes, train_loader, test_loader)"
      ],
      "metadata": {
        "id": "ul7rB65Qqorr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}